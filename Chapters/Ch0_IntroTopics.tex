%--------------------
%	CHAPTER 0
%--------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Preliminaries}
If you see a mistake in this document, or want further clarification somewhere, please open an issue on the book's \href{https://github.com/hilikliming/modern_algebra_with_friends}{\textul{github repository}}, or send me an email at jack.hall618@gmail.com \steezybreak \\
% \jjh{This is Jack's color}, \\
% \dts{This is Dan's color},\\
% \jsh{This is Jessie's color} \\ %% Use the command \jsh{ <insert comment here> } %% okie
% \jap{This is Jake's color}\\
% \kvdv{This is Kiki's color}
% (see lines 57-61 newcommand definitions in the main.tex for this document)\\ \\
\noindent 
\begin{tcolorbox}
    \begin{center}
\jjh{\href{https://www.dropbox.com/sh/759pwsjyc3ix0jy/AACv98R1Zpjvbaz2VhH4aH4ca?dl=0&fbclid=IwAR10ePsYI76S-MwDbBpH9fpoEbdxvyOlHs9PeaP2ZvY8SbJu1KPTxjMissA}{The Dropbox folder for this course can be found here. \\
Homework sets, a copy of this text (updated regularly), and homework solutions will be posted as we go through the material.}}\\
    \end{center}
\end{tcolorbox}


% \jjh{\href{https://youtube.com/playlist?list=PLW3u28VuDAHJNrf3JCgT0GG_rjFVz0-j9}{Another wonderful resource as you venture further into this material are Dr. Aviv Censor's lectures on Algebra (Algebra 1M Technion University). Dr. Censor covers many topics featured in this book and uses very similar conventions to our book. Thank you Dr. Censor and Technion U for making these videos public!!!}}\\
\bigbreak
\noindent Chapter 0 is meant to serve as a reference for symbols and terminology used in the main course material (Chapter 1 onward). On its own, this first Chapter is a doozy and we are going to throw all sorts of new notation at you. \textit{Don't be intimidated}. You do not need to understand or memorize all of this right away; the utility of these new notations will become apparent as you progress further through the text. Treat this chapter as a reference for the main course material that begins in Chapter 1. The authors recommend that readers who are familiar with mathematical induction already read sections \ref{sec:InductionExample} through \ref{sec:WhyInductionWorks}. For readers who have never seen induction before, as to not distract from the more important intuition, we would prefer you skip these and read \ref{sec:ChongInduction} first, then return to those sections after the underlying argument for the principle of induction is understood.

Lastly, when reading this material, read slowly and carefully and try to explain the argument out loud as if another were listening. Don't just read it aloud, really think about what the claim or statement is asserting. There is very little redundancy in mathematical writing so it pays dividends to read slowly, carefully, and to re-read many times until the argument is so clear that you could write it down again in your own words. When I read a new book or article in my field, the first 2-3 reads through I am just letting the new notations, jargon, definitions, and claims wash over me, I want to see the way the author uses these terms first and then I try to understand them more carefully. You can do it, you can piece it together! Just take baby steps to make your foundation solid. \newpage

\section{Common Notation seen in Herstein's book and this book}
This section presents commonly used mathematical symbols grouped into three categories: logical symbols, set-theoretic symbols, and proof-structuring symbols.

\subsection{Logic Notation: Quantifiers, Implication, and Falsum/Contradiction}

\begin{center}
\begin{tabular}{|c|l|}
    \hline
    \textbf{Symbol} & \textbf{Meaning and Reading} \\
    \hline
    $\forall$ & Universal quantifier, read as ``for all". \index{$\forall$}\\
    $\exists$ & Existential quantifier, read as ``there exists". \index{$\exists$} \\
    $\implies$ & Read as ``implies'' \index{$\implies$}\\
    $A \implies B$ & Implication, read as ``$A$ implies $B$", ``if $A$ then $B$", or ``$A$ only if $B$". \\
    $\iff$ & Logical equivalence, read as ``if and only if" (iff), ``is equivalent to". \index{$\iff$}\\
    $\lnot A$ & Negation, read as ``not A". \index{$\lnot$}\\
    $\lnot B \implies \lnot A$ & Contrapositive of $A \implies B$. \\
    $\Rightarrow\Leftarrow$ & Denotes a contradiction. Read as ``contradiction". \index{$\Rightarrow \Leftarrow$} \\
    \hline
\end{tabular}
\end{center}

\paragraph{Necessary and Sufficient Conditions}
$A \implies B$ can also be interpreted as ``$A$ is sufficient for $B$" or ``$B$ is necessary for $A$". If both $A \implies B$ and $B \implies A$ hold, then $A \iff B$, meaning $A$ and $B$ are equivalent. In this case, we also say that ``$A$ is necessary and sufficient for $B$" and vice versa.

\paragraph{Different Ways to Read $A \implies B$}
$A \implies B$ can be read as any of the following equivalent ways: ``$A$ implies $B$", ``if $A$ then $B$", ``$A$ only if $B$"\footnote{This is important, for further clarification, see this resource: \href{https://criticalthinkeracademy.com/courses/propositional-logic/lectures/51574}{here (link)}}. We sometimes also say "$A$ is \textit{sufficient} for $B$" or "$B$ is \textit{necessary} for $A$". \\

An example may help illuminate the equivalence between these five interpretations: Let $A=$``the match is burning'' and $B=$ ``there is oxygen in the room'', each of these five expressions tell us the same thing (namely, $A \implies B$) \footnote{Note that swapping $B$ and $A$ gives a wholly different meaning, who is to say that just because there is oxygen the match ought to be lit? They mean entirely different things.}:
\begin{enumerate}
    \item The fact that the match is burning, implies there is oxygen in the room.
    \item If the match is burning, then there must be oxygen in the room.
    \item The match is burning only if there is oxygen in the room.
    \item The fact that the match is burning is sufficient information to conclude that there is oxygen in the room.
    \item The presence of oxygen in the room is necessary for the match to be burning.
\end{enumerate}
Notice the difference in usage between ``if'' in 2. and ``only if'' in 3.
\begin{center}
    if (antecedent) then (consequent)  \\
(antecedent) only if (consequent) \\
\end{center}

\noindent The typical method of proof for proving a statement of the form ``if $A$ then $B$'' is to first assume $A$ is true and show that $B$ can be derived from that assumption.

\noindent Note that a statement of the form ``if $A$ then $B$'' does not say $A$ is true, nor does it say $B$ is true, it simply says \textit{if} $A$ can be shown to be true, \textit{then} we can conclude $B$ is true. 

%In this way, any conditional of the form ``if $A$ then $B$'' for which the antecedent $A$ is always false, is, somewhat non-intuitively (and trivially), \textit{true}. For example the statement ``if $0=1$ then Jack is a monkey's uncle" is (trivially) \textit{true} because the antecedent is always false. 

% In classical logic, the truth table of the if-then/implication can be summarized as:
% \begin{align*}
%     \begin{array}{c|c|c}
%         A & B & A \implies B \\
%         \hline
%         T & T & T \\
%         T & F & F \\
%         F & T & T \\
%         F & F & T \\
%         \end{array}
% \end{align*}


\paragraph{Different Ways to Read $A \iff B$}
$A\iff B$ can also be read as ``$A$ if and only if $B$", ``$B$ if and only if $A$", ``$A$ iff $B$", ``$B$ iff $A$", ``$A$ is equivalent to $B$", ``$B$ is equivalent to $A$". Lastly, this expression could also be read `` $A$ implies $B$ and $B$ implies $A$" or "$B$ implies $A$ and $A$ implies $B$". We emphasize the last two interpretations because iff statements (i.e. statements that use $\iff$) should be thought of as pairs of implications, one implication $LHS\implies RHS$ and another implication $RHS \implies LHS$. (LHS is left-hand-side here.)\\

\noindent The method of proof for proving a statement of the form ``$A$ if and only if $B$'' is to first assume $A$ is true and show that $B$ follows (can be derived from that assumption); and then assume $B$ is true and show that $A$ follows. That is we combine a proof of $A\implies B$ with a proof of $B \implies A$, these two together are proof of $A\iff B$.

\paragraph{Equivalence Between an Implication and Its Contrapositive}  
The contrapositive of an implication \( A \implies B \) is the statement \( \lnot B \implies \lnot A \), which is obtained by taking the contraposition of the original implication. In classical logic we have that  
\[
    (\lnot B \implies \lnot A) \iff (A\implies B)
\]  
(i.e., these expressions are always equivalent). This is often useful for proofs—rather than proving \( A\implies B \) directly, we can establish the same result by proving its contrapositive \( \lnot B \implies \lnot A \).  

The intuition for the equivalence between an implication and its contrapositive consists of two parts:  
\begin{enumerate}  
    \item ``If \( B \) is \textit{always} true when \( A \) is true (\( A\implies B \)), then when \( B \) is false, \( A \) must also be false (\( \lnot B \implies \lnot A \)).''  
    \item ``If \( A \) is \textit{always} false when \( B \) is false (\( \lnot B \implies \lnot A \)), then when \( A \) is true, \( B \) must be true (\( A\implies B \)).''  
\end{enumerate}  
Examples of proof by contrapositive and proof by contradiction are given in the sub-sections below.  

\paragraph{Conditional Statements}
A sentence of the form ``if $A$ then $B$'' (denoted $A \Rightarrow B$) is itself a statement. Just as statements $A$ and $B$ can be true or false, the statement $A \implies B$ can also be true or false. Similarly, a sentence of the form ``$A$ if and only if $B$'' (denoted $A \iff B$) is also a statement whose truth value can be evaluated. 

For example, let $A$ be ``Charlie is a human being'' and $B$ be ``Charlie was born on planet Earth.'' Then both $A \implies B$ and $A \iff B$ are statements that can be evaluated for truth. 
For the sake of argument we should note that to date, it is true that all humans have been born on the planet earth \footnote{At least it is true as of February 24th, 2025; we have yet to see a human being born anywhere but on planet earth, perhaps this will be a poor example in the future!}. Given this context, the first of these statements ($A\implies B$) is always true; if we assume that Charlie is a human, then since he is a human, he was certainly born on planet earth. However the second of these statements isn't necessarily true... sure the left hand side implies the right hand side (as before) but from the right hand side \textit{alone} can we infer that Charlie is a human? What if Charlie were a dog born on planet earth? Surely that does not entail that Charlie is a human! Without more context about Charlie, $B$ does not imply $A$ in general, so $A \iff B$ is false.\footnote{Though $A\implies B$ holds, $B\implies A$ does not, and both must be true for $A \iff B$ to be true.}


\subsection{Set Theory Notation}

\begin{center}
\begin{tabular}{|c|l|}
    \hline
    \textbf{Symbol} & \textbf{Meaning and Reading} \\
    \hline
    $a \in A$ & ``$a$ is an element of $A$" or ``$a$ in $A$". \index{$\in$} \\
    $a \not\in A$ & ``$a$ is \textit{not} an element of $A$". \index{$\not\in$} \\
    $\ni$ & ``For which" or "who exhibits the property...". \index{$\ni$}\\
    $|$ & ``For which" (used in set-builder notation). \\
    $a \in A \ni P(a)$ & ``$a$ is an element of $A$ for which $P(a)$ is true". \\
    \hline
\end{tabular}
\end{center}

\begin{tcolorbox}
\begin{center}
\textbf{NOTE:} In this book, \hl{$\in \neq \ni$}, meaning "element of" and "element-wise for which" are \textit{not} the same symbol and \underline{\hl{DO NOT mean the same thing}}.
\end{center}
\end{tcolorbox}

\noindent The vertical bar  $| \ \ $ also read as ``for which" or "who exhibit the property..." or "who have the property..." is almost always used when referring to \textit{collections/sets} of elements "for which" a property holds (i.e. not element-wise), it commonly appears in set builder notation (to be introduced in section \ref{sec:BasicSetTheory}) e.g. 
\begin{align}
    \{x\in \Z | 22\leq x < 27\}  \nonumber \\
    &= \{ \text{the subset of $\Z$ for which members are $\geq 22$ AND $<27$} \} \nonumber \\
    &= \{22,23,24,25,26\}\nonumber
\end{align}

\subsection{Proof Structure Symbols}

\begin{center}
\begin{tabular}{|c|l|}
    \hline
    \textbf{Symbol} & \textbf{Meaning and Reading} \\
    \hline
    $\blacksquare$ & Denotes the end of a proof. Some authors use $\square$. \index{$\blacksquare$}\\
    $\therefore$ & read as ``Therefore". \index{$\therefore$}\\
    $\because$ & read as ``Because" (used rarely). \index{$\because$} \\
    \hline
\end{tabular}
\end{center}
\newpage
\section{Logical Connectives and Their Truth Tables}

In propositional logic, statements (or \emph{propositions}) can be combined using
\emph{logical connectives}. We denote \emph{conjunction} (logical ``and'') by the symbol $\land$,
and \emph{disjunction} (logical ``or'') by the symbol $\lor$.
Given two propositions $P$ and $Q$:

\begin{itemize}
    \item $P \land Q$ is \emph{true} if and only if \emph{both} $P$ and $Q$ are true.
    \item $P \lor Q$ is \emph{true} if at least one of $P$ or $Q$ is true.
\end{itemize}

\noindent Below are the truth tables for the standard logical connectives:

\begin{center}
\begin{tabular}{c c | c c c c c c}
$P$ & $Q$ & $\lnot P$ & $P \land Q$ & $P \lor Q$ & $P \Rightarrow Q$ & $P \Leftrightarrow Q$ & $\lnot P \lor Q$ \\
\hline
T & T & F & T & T & T & T & T \\
T & F & F & F & T & F & F & F \\
F & T & T & F & T & T & F & T \\
F & F & T & F & F & T & T & T \\
\end{tabular}
\end{center}

\noindent Here:
\begin{itemize}
    \item $\lnot P$ (\emph{negation}) is true when $P$ is false.
    \item $P \Rightarrow Q$ (\emph{implication/if-then}) is false only when $P$ is true and $Q$ is false.
    \item $P \Leftrightarrow Q$ (\emph{biconditional/if-and-only-if}) is true when $P$ and $Q$ have the same truth value.
\end{itemize}

\noindent Here's how to read this table, on the left we have two propositions $P$ and 
$Q$ which might be true and might be false. Depending on their truth value, the 
expressions on the right side of the table take on different truth values. For example, 
$\lnot P$, the negation of $P$ always takes on the opposite truth value of $P$ itself. 
If we look at the conjunction of $P$ and $Q$ (denoted $P\land Q$) we see that $P$ and 
$Q$ is only true when both $P$ is true and $Q$ is true. On the other hand $P\lor Q$ is 
true when either $P$ or $Q$ (or both) are true. \steezybreak \\

\noindent It should be noted that an equivalent form of $P \Rightarrow Q$ is $\lnot P \lor Q$ (note how they have identical truth tables).


\section{Negating Logical Connectives and Quantifiers}
This table provides a resource for negating the most common types of expressions that we will encounter in this class:
\begin{center}
  \begin{tabular}{|c|c|}\hline
     \textbf{Statement: $S$} & \textbf{Negation of Statement: $\lnot S$}   \\ \hline
     $A$ and $B$ & $\lnot A$ or $\lnot B$  \\ \hline
     $A$ or $B$ & $\lnot A$ and $\lnot B$  \\ \hline 
     if $A$ then $B$ & $A$ and $\lnot B$  \\ \hline
     $\forall \ x, \ P(x)$ & $\exists \ x \ni \lnot P(x)$  \\ \hline
     $\exists \ x \ni \ P(x)$ & $\forall \ x, \lnot P(x)$  \\ \hline
\end{tabular}  
\end{center}
The first two rows in this table are often referred to as DeMorgan's Laws 1 \& 2 respectively (or sometimes DeMorgans ``and'' negation law and Demorgan's ``or'' negation law.) \steezybreak

\noindent The reader should also take a moment to meditate on the negations of the universal and existential quantifiers. It should agree with your intuition that if we can't say \textul{all} $x$'s obey $P$, then there must exist an $x$ for which ``not $P$'' holds. Similarly if there doesn't exist a single $x$ for which $P$ holds, then forall $x$ we have ``not $P$''.
\newpage 
\section{A Word About Axioms in This Book}
\label{sec:axioms}
As of January 11th, 2021, Merriam-Webster defines the noun ``Axiom" as:
\begin{enumerate}
    \item ``a statement accepted as true as the basis for argument or inference"
    \item ``an established rule or principle or a self-evident truth".
    \item ``a maxim widely accepted on its intrinsic merit". Where, here, maxim means ``a general truth, fundamental principle, or rule of conduct".
\end{enumerate}
In the land of Mathematics, the meaning of the word ``Axiom" most closely resembles the definition given in 1. ; i.e. Axioms are statements which are assumed to be true and used as the basis for argument or inference \textit{without} justification. 

This book is not a set theory or foundations of mathematics course so we will take a \textit{few} things for granted. In this book, we will not attempt to define or construct the most fundamental tools that we begin our studies with: Sets and Integers (the set $\Z$ and properties of its elements). We will instead take these to be primitive notions that do not need justification. You can take a set to be a collection of objects, \textit{with no repeated objects}\footnote{Sets are defined by the elements they contain, an element either \textit{is} in a given set or it \textit{is not}, each element is unique, there is no notion of duplicity or "multiple copies" of elements, so while you might write out a set like $\{1, 2, 2, 3\}$ this is the same as the set $\{1,2,3\}$ and we prefer to write sets in the latter of these two ways for clarity.}. You can also take the following facts about integers as axioms:
\noindent\begin{align}
    a,b\in \Z &\implies a+b\in \Z  \ \ \ &\text{i.e. it is always true that the sum of two integers is an integer}\nonumber \\
    \text{ and } &\implies a-b\in \Z \ \ \ &\text{i.e. it is always true that the difference of two integers is an integer}\nonumber \\
    \text{ and } &\implies \exists \ -a,-b \in \Z \ \ \ &\text{i.e. every integer has an additive opposite or "negative" that is an integer}\nonumber \\
     \text{ and } &\implies a\cdot b\in \Z \ \ \ &\text{i.e. the product of two integers is always an integer}\nonumber
\end{align} 
Further, the fundamentals of grade-school algebra, such as the manipulations mentioned in Section \ref{sec:ManipulatingInequalities}, can be used without providing justification.

All of this being said, using the tools of first order logic that you will learn along the way, if you wished to, you could ``begin again" with some weaker assumptions and build your way back up to showing that e.g. adding, subtracting, or multiplying two integers gives another integer, or that every integer $a\in\Z$ has an opposite $-a\in\Z$. 

\section{Basic Set Theory Definitions}
\label{sec:BasicSetTheory}
Given a set $S$ we will consistently use the notation $A=\{a\in S\ |\ P(a)\}$ to read `` $A$ is the set of all elements in $S$ for which the property $P$ is true". This notation is commonly referred to as \textbf{set builder} notation \index{set builder notation}. We read the vertical bar `` $|$ " as ``for which" or sometimes ``such that", we will also commonly use the symbol $\ni$ to indicate ``for which" or ``such that" when we are referring to individual elements, e.g. $a\in A \ni a>0$ is read as `` $a$ is an element of $A$ for which $a>0$" or ``$a$ is an element of $A$ such that $a$ is greater than zero". \\ \\
\noindent Here is a simple example of this \textbf{set builder} notation in use, let $P(a)=$ "$a \text{ is even}$". \\
Then $\{a\in \Z | P(a)\} = \{...,-4,-2,0,2,4,6,...\} = \{ \text{the set of all even integers}\}$ \\ \\
Sometimes, in other math texts, a colon, ``$:$", will be used in place of the vertical bar, ``$|$", in set builder notation.\\ \\
\noindent We will now present thirteen basic, but very important, definitions from Set Theory. \newpage
\begin{definition}[Subset]\index{$\subset$}
Set $A$ is a \textit{subset} of set $B$, denoted $A\subset B$, if every element of $A$ is an element of $B$, that is, if
\begin{align}
    x\in A \implies x\in B \nonumber
\end{align}
$A\subset B$ is typically read as "$A$ contained in $B$" or "$B$ contains $A$" or "$A$ is a subset of $B$"
\end{definition}
A simple visual example of $A\subset B$ can be seen in Figure \ref{fig:Subset_example}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/SUBSET_Example.pdf}
    \caption{$A$ is a subset of $B$ since each element of $A$ is an element of $B$.}
    \label{fig:Subset_example}
\end{figure}\\
To define a set, we start with some “universe” of all possible elements of that set, usually called the \textit{universe of discourse} or simply \textit{universe}. In the figure above, our universe is denoted by $\Omega$. Then, a set $S$ can be defined by specifying which elements of the universe of discourse is in that set $S$, typically done with the set builder notation that we just introduced.
\begin{definition}[Set equality]
Sets $A$ and $B$ are equal if both $A\subset B$ and $B\subset A$
\end{definition}

\begin{definition}[Union of Sets] \index{$\cup$}
The \textit{union} of the two sets $A$ and $B$, written as $A\cup B$, is the set
\begin{align}
    A\cup B = \{x\in \Omega |x\in A \text{ or } x\in B \} \nonumber
\end{align}
\end{definition}
Let's read the above definition of $A\cup B$ in more regular speech, this equation says "$A$ union $B$ equals the set of elements $x$ for which $x$ is a member of $A$ \textbf{or} $x$ is a member of $B$". We are using "$x$" here as a \textit{variable} and we mean that any such member $x$ who is either in $A$ \textbf{or} in $B$ (or possibly both!) is a member of this set $A$ union $B$. \\ \\
Figure \ref{fig:set_union_example} depicts a the union of two sets, $A$ and $B$.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Set_Union.pdf}
    \caption{The shaded region is $A\cup B$. $A\cup B$ is the set of all elements that are either in $A$ or in $B$.}
    \label{fig:set_union_example}
\end{figure}
\begin{definition}[Intersection of Sets]\index{$\cap$}
The \textit{intersection} of the two sets $A$ and $B$, written as $A\cap B$, is the set
\begin{align}
    A\cap B=\{x \in \Omega |x\in A \text{ and } x\in B \} \nonumber
\end{align}
\end{definition}
Let's read the above definition of $A\cap B$ in more regular speech, this equation says "$A$ intersect $B$ equals the set of elements $x$ for which $x$ is a member of $A$ \textbf{and} $x$ is a member of $B$". Again, we are using $x$ here as a variable and we mean that any element (who we choose to represent with the "handle" $x$) who satisfies $x$ in $A$ \textbf{and} $x$ in $B$ is a member of this set $A$ intersect $B$.\\ \\

\noindent Figure \ref{fig:set_intersection_example} depicts a the intersection of two sets $A$ and $B$.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Set_INTERSECTION.pdf}
    \caption{The shaded region is $A\cap B$. $A\cap B$ is the set of all elements that are both in $A$ \textit{and} in $B$.}
    \label{fig:set_intersection_example}
\end{figure}

\begin{definition}[Difference of Sets (Difference Set)]
Given two sets $A$ and $B$, then the \textit{difference set} written as $A - B$ or sometimes $A\setminus B$, is the set
\begin{align}
    A-B=\{x\in A| x\not\in B\}=\{x\in \Omega |x\in A \text{ and } x\not\in B \} \nonumber
\end{align}
\end{definition}
Note that we could define the difference set $A- B$ %lightly more generally by defining it in terms of some ``universe" set $\Omega$ who contains all the sets we are considering as subsets, i.e. $A,B \subset \Omega$. 
in another way using an idea known as set complement. In this case we would have $A- B=A\cap B^C$ where $B^C$ is referred to as the complement of set $B$ and it is defined as $B^C=\{x\in \Omega| x\not \in B\}$. For this reason, $A-B$ is sometimes called "the complement of $B$ in $A$". This form will almost never come up in this text but we mention it for interested readers who may go on to study Measure Theory (i.e. Probability), as it is an extremely convenient way to rewrite the difference of two sets when dealing with proofs in Measure Theory. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Complements View of Set Minus.pdf}
    \caption{Complements View of Set Minus: on the left side of the intersection, set $A$ is shaded in, on the right side of the intersection set $B^C$ ($B$'s complement in universe $\Omega$) is shaded in; now it is easy to see that $A$'s intersection with $B^C$ (i.e. the portion these two shaded regions have in common) is simply $A-B$, or "the parts of $A$ that are not in $B$".}
    \label{fig:complements_view_set_minus}
\end{figure}
\newpage
\noindent For the two definitions provided below, the reader should note a finite union/intersection is the union/intersection of a finite number of sets; this phrase does not imply that the union/intersection set itself is a finite set!

\begin{definition}[Finite Union of Sets]
    A \textit{finite union} of sets $A_1, A_2, A_3, \dots , A_n$ could be written $A_1 \cup A_2 \cup A_3 \cup \dots \cup A_n$ (as union is associative) or, more commonly, $\bigcup_{i=1}^n A_i$. 
\begin{align}
    \bigcup_{i=1}^n A_i &= \{x\in \Omega \ | \ x\in A_1 \text{ or } x \in A_2 \text{ or } x\in A_3 \  \cdots \text{ or } x\in A_n\} \nonumber \\
    &= A_1 \cup A_2 \cup A_3 \cup \dots \cup A_n \nonumber 
\end{align}
%\textit{The second equality above can be proven using an induction argument and associativity of unions (which allows us to remove parentheses without ambiguity), some might enjoy proving this as an exercise, but it is not required. Same for the definition below and associativity of intersections}
\end{definition}

\begin{definition}[Finite Intersection of Sets]
    A \textit{finite intersection} of sets $A_1, A_2, A_3, \dots , A_n$ could be written $A_1 \cap A_2 \cap A_3 \cap \dots \cap A_n$ (as intersection is associative) or, more commonly, $\bigcap_{i=1}^n A_i$. 
\begin{align}
    \bigcap_{i=1}^n A_i &= \{x\in \Omega \ | \ x\in A_1 \text{ and } x \in A_2 \text{ and } x\in A_3 \ \cdots \text{ and } x\in A_n\} \nonumber \\
    &= A_1 \cap A_2 \cap A_3 \cap \dots \cap A_n \nonumber 
\end{align}
\end{definition}

\begin{definition}[Disjoint Sets]
    Sets $A, B$ are said to be disjoint if $A\cap B= \emptyset=\{\}$\steezybreak \\
    \noindent That is, a pair of sets are disjoint if they share no elements in common (hence, their intersection is the empty set)
\end{definition}

\noindent Figure \ref{fig:disjoint_sets} depicts an example of two sets $A$ and $B$ that are \textit{disjoint}, that is, they share no elements and their intersection is the empty set.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/disjoint_sets.pdf}
    \caption{$A$ and $B$ have no elements in common so $A\cap B=\emptyset$.}
    \label{fig:disjoint_sets}
\end{figure}

\begin{definition}[Mutually Disjoint (Collection of) Sets]
    A collection of sets $A_1,A_2,...,A_n$ are said to be mutually disjoint if $A_i\cap A_j=\emptyset$ whenever $i\neq j$ \\

    \noindent That is, a collection of sets are mutually disjoint when each set in the collection contain no common elements with any other set in the collection (aside from itself).
\end{definition}
\newpage
Without going into too much detail we want to briefly introduce the concept of cardinality. Cardinality describes a relationship between sets which compares their relative size.
\begin{definition}[Cardinality (of a Set)]
    The \textit{cardinality} of a set \(A\), denoted \(|A|\), is, roughly speaking, the "number of elements" in \(A\). \\

    \noindent For finite sets, \(|A|\) is the total count of elements in \(A\). For example, if \(A = \{1, 2, 3\}\), then \(|A| = 3\). \\
    \noindent As another example, if \(B=\{green, red, blue, red, orange\}\) then $|B|=4$.\footnote{Recall the footnote from Section \ref{sec:axioms}.}\\ 
    
    \noindent \textbf{A More Complete Definition\footnote{Revisit after you have begun Chapter 1 material}} 

    \noindent For sets in general, \textit{cardinality} is determined by comparing a set \(A\) to another set \(B\) using a \textit{bijection} \footnote{To be covered in the next chapter, see Definition \ref{def:surj_inj_bij}}, which is a one-to-one correspondence between the elements of \(A\) and \(B\):
    \begin{align}
        |A| &= |B| \quad \text{if and only if there exists a bijective function } f: A \to B. \nonumber \\
        |A|&\leq |B| \quad \text{if there exists an injective function }f:A \to B. \nonumber \\
        |A|&<|B| \quad \text{if there exists an injective function } f:A \to B \quad \text{but no bijective function.} \nonumber
    \end{align}
\end{definition}
% Add as a footnote???
% After reading the simplified and complete definition you may think to yourself "in the simple definition cardinality seems like a fixed property of a set, no comparisons needed, in the more complete definition it's all about comparison between pairs of sets? what gives?"
% Well these are two perspectives on the same concept:
% Fixed Property of a Set: The cardinality of a set \textit{is indeed} a fixed property—it represents, in a sense, "how many elements" the set contains or the "size" of the set. For finite sets, this is straightforward: you simply count the elements. 
% For example, $|A|=3$ if $A=\{1,2,3\}$. 

% Comparison Perspective: For infinite sets, we can't directly "count" elements the way we do for finite sets. Instead, cardinality is determined by comparing one set to another using bijections and injections (one-to-one correspondences, and one-to-one maps). 

% For example, the set of integers  $\Z$ and the set of natural numbers $\N$ have the same cardinality because you can establish a bijective function between them.
% Its easier to see how these two perspectives related by considering finite sets versus infinite sets:
% For finite sets, these two perspectives collapse into one: you can directly count the elements, and the existence of a bijection between two finite sets corresponds exactly to their element counts being equal.
% For infinite sets, the "comparison perspective" becomes essential because there’s no direct way to \textit{count} their elements. This comparison through bijections allows us to define and compare their cardinalities rigorously. It gives us a way to say this guy is bigger than this guy even when both are infinite!

\noindent The simplified definition applies only to finite sets but helps build intuition about cardinality as the "size" of a set. The more rigorous definition extends this concept to both finite and infinite sets, enabling us to compare sets such as the natural numbers \(\mathbb{N}\) (\(|\mathbb{N}| = \aleph_0\), pronounced "aleph nought") and the real numbers \(\mathbb{R}\) (\(|\mathbb{R}| = 2^{\aleph_0} > \aleph_0\)). \\ \steezybreak

\noindent The rigorous definition also aligns with our intuition for finite sets: two finite sets have the same number of elements if and only if a bijection exists between them.

\begin{definition}[Cartesian Product (of Sets)]
    Given sets $A$ and $B$, the \textit{Cartesian product} of $A$ and $B$ is the set of all ordered pairs $(a,b)$ where $a\in A$ and $b\in B$, that is 
    \begin{align}
        A\times B &= \{(a,b) \ | \ a\in A \text{ and } b\in B\} \nonumber
    \end{align} 
\end{definition}
Consider the set $A=\{a,b,c\}$ and the set $B=\{1,2\}$, then $A\times B =\{(a,1), (a,2), (b,1), (b,2), (c,1), (c,2)\}$. If the cardinality (i.e. the number of elements contained within a set) is finite for both $A$ and $B$ then the cardinality of the cartesian product $A\times B$ is the product of the individual cardinalities, i.e. $|A\times B| = |A|\cdot |B|$. The set $A\times B$ is infinite if either $A$ or $B$ is infinite and the other set is non-empty.
\begin{definition}[n-ary Cartesian Product]
    The Cartesian product can be generalized to the \textit{$n$-ary Cartesian Product }over $n$ sets $A_1, A_2, \cdots, A_n$ as the set 
    \begin{align}
        A_1 \times \cdots \times A_n &= \{(a_1, \cdots, a_n ) \ | \ a_i \in A_i \text{ for every }i\in \{1, \cdots, n\}\} \nonumber
    \end{align}
\end{definition}

\begin{definition}[n-ary Cartesian power]
    The \textit{$n-$ary Cartesian Power} of a set $A$, denoted $A^n$ is defined
    \begin{align}
        A^n &= \underbrace{A\times A \times \cdots \times A}_{n} = \{(a_1, a_2, \cdots, a_n) \ | \ a_i \in A \text{ for every } i \in \{1, \cdots, n\}\} \nonumber
    \end{align}
\end{definition}
\newpage
\noindent\section{Some important sets to remember}
% \begin{align}
% \emptyset=\{\} =\text{ The empty set (set with no elements, $\emptyset$ is trivially a subset of all sets) } \nonumber \\
% \text{Bool}= \{T,F\} \nonumber \\
% \mathbb{Z}=\{..,-3,-2,-1,0,1,2,3,...\}= \text{ the integers} \nonumber\\
% \mathbb{Q}=\{\frac{a}{b} \ | \  a,b \in \mathbb{Z} \ni b\neq 0\}= \text{ the rationals} \nonumber\\
% \mathbb{R}=\{\ ...c_1c_0.c_{-1}c_{-2}...\  | \ c_i\in \{0,1,2,3,4,5,6,7,8,9\} \ \forall  i  \}= \text{ the reals} \nonumber\\
% \text{sets } A, B \text{ are said to be disjoint if } A\cap B= \emptyset=\{\} \nonumber
% \end{align}

$\emptyset=\{\}$ The empty set (set with no elements, $\emptyset$ is trivially a subset of all sets since for any set, empty or non-empty, you can always consider the collection of \textit{none} of its elements, this is exactly the empty set) \index{$\emptyset$} \index{$\{\}$}\\
%$\text{Bool}= \{T,F\}$ \\
$\mathbb{N}=\{0, 1,2,3,4,...\}$  = the set of natural numbers\\ 
$\mathbb{Z}=\{..,-3,-2,-1,0,1,2,3,...\}=$ the set of integers\\
$\mathbb{Q}=\{\frac{a}{b} \ | \  a,b \in \mathbb{Z} \ni b\neq 0\}$= the set of rational numbers \\
$\mathbb{R}=\{\ \pm \ \dots c_1c_0.c_{-1}c_{-2}\dots \  | \ c_i\in \{0,1,2,3,4,5,6,7,8,9\} \ \forall  \ i  \ \}=$ the set of real numbers

\noindent In this final description of the real numbers $\R$ we want to point out:
\begin{itemize}
    \item The \(\pm\) indicates that the number can be positive or negative.
    \item \(c_i\) are the digits of the decimal (base 10) expansion. It is customary to omit trailing and leading $0$'s.
    \item The index \(i\) represents the position of each digit relative to the decimal point, with \(i=0\) corresponding to the ones place, \(i=-1\) corresponding to the tenths place and so on.
    \item When using decimal expansions to define the reals you must take care; realize that a single element may go by more than one representation, e.g. $0.999 \ldots = 1.000 \ldots$ and $2.999 \ldots = 3.000 \ldots$ This ambiguity reflects the fact that decimal expansions are not unique for certain real numbers.
    % \footnote{This will occur in any base, not just base $10$ (decimal). For example, in base $3$ we would write a number like $5.333...$ as $12.1000= (1*3^1 + 2*3^0 + 1*3^{-1})$. $4$ looks like $11.0=(1*3^1 + 1*3^0)$.
    % Our coefficients in front of the powers of $3$ never exceed $2$ (they are $0$, $1$, or $2$) just like in decimal we get to work with $0, 1, 2, ... ,9$ before we have to bring in a new power of $10$
    % But, importantly, the number $0.5$ (whose representation is not unique in base $10$) suddenly has a unique representation here where we represent our reals in ternary, the only acceptable representation is 
    % $0.11111...$ 
    % (If you want to confirm that $0.111...$ in ternary is $0.5$ in decimal, recall from Calc II the geometric series $\sum_{n=0}^\infty ar^n$ where $r<1$, this series converges to $\frac{a}{1-r}$, The number we wrote is $1*3^{-1} + 1*3^{-2} + ...$
    % Which looks an awful lot like a geometric series but just missing the $0^{th}$ term. So using that geometric series with $r=\frac{1}{3}$ and $a=1$ we get $\frac{1}{2/3}= \frac{3}{2}$. Now since our number $0.11111...$ is just that sum from $n=1$ to infinity we can subtract the $a*r^0=a=1$ term and we get $3/2-1=1/2$)
    % in a similar vein... $1/3$ doesn't have a unique representation in ternary, it could be written $0.10000$ or it could be written $0.0222...$ but in decimal, there is only one way to write it: $0.333...$
    % }
    \footnote{This occurs in any base, not just base $10$. For instance, in base $3$, $5.333\ldots$ becomes $12.1000$ $(1 \cdot 3^1 + 2 \cdot 3^0 + 1 \cdot 3^{-1})$, and $4$ is $11.0$ $(1 \cdot 3^1 + 1 \cdot 3^0)$. Digits in base $3$ are $0$, $1$, or $2$, similar to decimal digits $0$ through $9$. 
    Notably, $0.5$—which is not uniquely represented in base $10$—has a unique ternary representation: $0.111\ldots$ Using the geometric series formula $\sum_{n=0}^\infty ar^n = \frac{a}{1-r}$ with $a=1$ and $r=\frac{1}{3}$, $0.111\ldots = \frac{3}{2} - 1 = \frac{1}{2}$ (we subtract $1=ar^0$ because our number is $\sum_{n=1}^\infty ar^n$). Conversely, $1/3$ lacks a unique 
    ternary form: it could be $0.1000$ or $0.0222\ldots$, whereas in decimal, $0.333\ldots$ is unique.}
    \item Here we provide a representation of the real numbers via decimal expansions. However, \(\mathbb{R}\) can also be rigorously defined using other constructions, such as Cauchy sequences or Dedekind cuts.
    \item In simpler terms, The real numbers $\R$ are the set of \textit{all} points on the number line, including both rational numbers (fractions) and irrational numbers (non-repeating, non-terminating decimals).
\end{itemize}
\subsection{Interval Notation for Interval Subsets in $\R$}
We will use the following standard interval notation for \textit{interval subsets} of $\R$.\\ 
Let $a,b\in \R, a<b$ we denote the following sets:
\begin{align}
    (a,b)&= \{r\in \R \ | \ a<r<b \} \subset \R \nonumber \\
    [a,b)&= \{r\in \R \ | \ a\leq r<b \} \subset \R \nonumber \\
    (a,b]&= \{r\in \R \ | \ a< r\leq b \} \subset \R \quad \text{and lastly, } \nonumber \\
    [a,b]&= \{r\in \R \ | \ a\leq r \leq b \} \subset \R \nonumber
\end{align}
% $(a,b)= \{r\in \R \ | \ a<r<b \} \subset \R$,\\
% $[a,b)= \{r\in \R \ | \ a\leq r<b \} \subset \R$, \\ 
% $(a,b]= \{r\in \R \ | \ a< r\leq b \} \subset \R$, and lastly, \\
% $[a,b]= \{r\in \R \ | \ a\leq r \leq b \} \subset \R$ \\ 
\noindent A key observation about the above interval notations is that square brackets, "$[$" or "$]$", are used when a given "endpoint" (i.e. $a$ or $b$) \textit{is} a member of the subset. Round brackets, "$($" or "$)$", are used when a given "endpoint" \textit{is not} a member of the subset. From examples above this means $a\not \in (a,b]$ but $b\in (a,b]$ and similarly $a,b\in [a,b]$, but $a\not \in (a,b)$ and $b \not \in (a,b)$.
\newpage

\section{Valid Algebraic Manipulations of Inequalities}
\label{sec:ManipulatingInequalities}

The following table summarizes some algebraic transformations that preserve the truth of inequalities over the real numbers (and hence hold for inequalities of elements from any subset of the reals). This is by no means an exhaustive list but contains some useful ones that should be familiar from gradeschool and high school mathematics. All variables are assumed to be real unless otherwise noted. \steezybreak \\

\noindent Note: Any implication stated for $a \ge b$ also holds for the stricter case 
$a > b$ (and trivially for $a = b$). Therefore, it is unnecessary to list both forms; 
the $\ge$ case subsumes the $>$ case.

\renewcommand{\arraystretch}{1.4} % Increase vertical spacing
\vspace{-0.2in}
\begin{center}
\begin{tabular}{|p{3.5cm}|p{11.1cm}|}
\hline
\textbf{Rule/Transformation} & \textbf{Valid Implications} \\
\hline
Transitivity &
\( a \geq b \text{ and } b \geq c \implies a \geq c \) \\
%& \( a > b \text{ and } b > c \implies a > c \) \\
\hline
Addition/Subtraction &
\( a \geq b \implies a + c \geq b + c \quad \text{for all } c \in \mathbb{R} \) \\
%& \( a > b \implies a + c > b + c \quad \text{for all } c \in \mathbb{R} \) \\
\hline
Multiplication &
\( a \geq b \text{ and } c > 0 \implies ac \geq bc \) \\
& \( a \geq b \text{ and } c < 0 \implies ac \leq bc \) \\
%& \( a > b \text{ and } c > 0 \implies ac > bc \) \\
%& \( a > b \text{ and } c < 0 \implies ac < bc \) \\
\hline
Reciprocals &
\(a>0 \implies \frac{1}{a}>0\) \\
&\( 0 < a \leq b \implies \frac{1}{a} \geq \frac{1}{b} \) \\
& \( a \leq b < 0 \implies \frac{1}{a} \geq \frac{1}{b} \) \\
%&\( 0 < a < b \implies \frac{1}{a} > \frac{1}{b} \) \\
%& \( a < b < 0 \implies \frac{1}{a} > \frac{1}{b} \) \\
\hline
Additive Bounds$^\dagger$ &
\( a_1 \leq b_1 \text{ and } a_2 \leq b_2 \implies a_1 + a_2 \leq b_1 + b_2 \) \\
%& \( a_1 < b_1 \text{ and } a_2 < b_2 \implies a_1 + a_2 < b_1 + b_2 \) \\
\hline
Multiplicative Bounds$^\dagger$ &
\( 0 \leq a \leq b \text{ and } 0 \leq c \leq d \implies ac \leq bd \) \\
%& \( 0 < a < b \text{ and } 0 < c < d \implies ac < bd \) \\
\hline
Division$^\dagger$ &
\( a \geq b \text{ and } c > 0 \implies \frac{a}{c} \geq \frac{b}{c} \) \\
& \( a \geq b \text{ and } c < 0 \implies \frac{a}{c} \leq \frac{b}{c} \) \\
%& \( a > b \text{ and } c > 0 \implies \frac{a}{c} > \frac{b}{c} \) \\
%& \( a > b \text{ and } c < 0 \implies \frac{a}{c} < \frac{b}{c} \) \\
\hline
Squaring (non-negative)$^\dagger$ &
\( 0 \leq a \leq b \implies a^2 \leq b^2 \) \\
%&\( 0 \leq a < b \implies a^2 < b^2 \) \\
\hline
Monotonic Functions$^{\dagger\dagger}$ &
\( f \text{ monotone increasing on domain } D \text{ and } a,b \in D \text{ with } a \geq b \implies f(a) \geq f(b) \) \\
& \( f \text{ monotone decreasing on domain } D \text{ and } a,b \in D \text{ with } a \geq b \implies f(a) \leq f(b) \) \\
%& \( f \text{ monotone increasing on domain } D \text{ and } a,b \in D \text{ with } a > b \implies f(a) > f(b) \) \\
%& \( f \text{ monotone decreasing on domain } D \text{ and } a,b \in D \text{ with } a > b \implies f(a) < f(b) \) \\
\hline
Absolute Value &
\( |a| \geq 0 \) \\
& \( |a| \leq b \iff -b \leq a \leq b \) \\
& \( |a| \geq b \text{ and } b \geq 0 \implies a \leq -b \text{ or } a \geq b \) \\
%& \( |a| < b \iff -b < a < b \) \\
%& \( |a| > b \text{ and } b \geq 0 \implies a < -b \text{ or } a > b \) \\
\hline
\end{tabular}
\end{center}
\vspace{-0.1in}
$\dagger$ These can be proven from the first four rules (Trans., Add., Mult., and Rec.), try it for yourself! \\
$\dagger\dagger$ This simply follows from the definition of monotone increasing/decreasing, some familiar monotone increasing functions include $\log(x)$ (whose domain is $D=\R^+ \subset \R$), $\exp(x)$ (whose domain is $\R$), and $x^3$ (whose domain is $\R$).

\newpage

\section{Proof by Contrapositive Example} 
\label{sec:ContrapositiveExample}
Let $x\in \mathbb{Z}$ and suppose we have statements $A=$``$x^2$ is even" and $B=$``$x$ is even", and wish to prove $A\implies B$, we can instead prove $\lnot B \implies \lnot A$ where $\lnot B=$``$x$ is not even" and $\lnot A=$``$x^2$ is not even". This latter statement is equivalent to the former and a proof by contraposition goes as follows: suppose that $x$ is not even, then $x$ is odd. The product of two odd numbers is odd, hence $x^2 = x\cdot x$ is odd. Thus $x^2$ is not even. Having proved the contrapositive, we can then infer that the original statement is true, i.e. that $A \implies B$ \\ \\
\href{https://www.math.toronto.edu/preparing-for-calculus/3_logic/we_3_negation.html}{Click Here for help with negating logical expressions. (See table at end of webpage)}\\

\noindent This is not to be confused with proof by contradiction. In proof by contradiction we have statement $A\implies B$ that we wish to show. To accomplish this, we begin by assuming $A$ is true \textit{and} that $B$ is false and we will arrive at a contradiction (denoted as $\Rightarrow\Leftarrow$) that is a direct consequence of our incorrect assumption about $B$, meaning the only possibility is that our assumption about $B$ being false when $A$ is true was incorrect, meaning that if $A$ is true then $B$ must be true too, giving us the desired result: $A \implies B$ 

\noindent\section{Proof by Contradiction (a.k.a. 
\textit{Reductio ad absurdum}) Example:} 
\label{sec:RAAExample}
In this example\footnote{ A reader who is familiar with intuitionistic logic may scoff at this example for Reductio ad absurdum as it relies on Indirect Proof (In some camps, RAA is the same as ``Proof of Negation" from Int. Logic, and what I called RAA here would be referred to as Indirect Proof). We will be working with Classical Logic for the time being, and under those rules, this is all fine. I will present the natural deduction rules we've been using ( see Dr. Mark Jago's excellent lectures here: \textcolor{orange}{\href{https://www.youtube.com/watch?v=dlUkeN7KqVA&t=8s}{Propositional Logic/Classical Natural Deduction}/ \href{https://www.youtube.com/watch?v=C30w5vZypXE}{First-Order Logic}}) towards the end of the text and allude to logics that relax assumptions like double negation elimination and indirect proof. Stick with us, you may enjoy the presentation of the algebraic topics covered here \smiley } we will prove an implication using \textit{Reductio ad absurdum} or proof by contradiction. Given statements $A= "x^2 \text{ is even}"$, and $B= "x \text{ is even}"$, we could show $A\implies B$ through the following contradiction: Assume $A$ true and $\lnot B$ true, that is assume ``$x^2 \text{ is even}$" and ``$x \text{ is not even}$", now since $x$ is odd, and the product of two odds is odd, $x\cdot x$ is odd, but $x\cdot x=x^2$ and $x^2$ was assumed even! $\Rightarrow\Leftarrow$ (contradiction!). What we have shown is that $\lnot B$ cannot be true when $A$ is true. So our only option left is to conclude that if $A$ is true, then $B$ is true, i.e. $A\implies B$

\section{Proof by Induction Example}
\label{sec:InductionExample}
Mathematical induction is a technique of proof that is essentially used to prove that a statement $P(n)$ holds for \textit{every} natural number $n = 0, 1, 2, 3, ...$ ; that is, the overall statement is a sequence of infinitely many cases $P(0), \ P(1), \ P(2), \ P(3), \ ...$ \\
\noindent A proof by induction consists of two cases. The first, the base case (or basis), proves the statement for $n = 0$ without assuming any knowledge of other cases. The second case, the induction step, proves that if the statement holds for any given case $n\geq 0$, then it must also hold for the next case $n + 1$. These two steps establish that the statement holds for every natural number $n$. The base case does not necessarily begin with $n = 0$, but often with $n = 1$, and possibly with any fixed natural number $n = N$, establishing the truth of the statement for all natural numbers $n \geq N$. Now we will give an example of this approach of proof in order to prove the following fact about sums of natural numbers:
\begin{align}
    \sum_{k=1}^{n} k = \frac{n(n+1)}{2} \ \ \ \forall n \in \N \nonumber
\end{align}
In the proof below we will begin by establishing the result for a \textit{base case} where $n=1$. Then, in the \textit{induction step}, we will \textit{assume} that it holds for some $n\geq 1$ and we will show that assuming this $\implies$ $n+1$ also satisfies the property. The base case, together with the induction step, show that the rule is obeyed by all natural numbers $n\geq 1$ in a domino sort of effect, since $n=1$ (base) case plus the induction step implies that $n=2$ holds, then $n=2$ plus induction step implies $n=3$ holds, and so on and so forth. The principle of induction is a formal statement of this intuitive reasoning. The proof begins on the next page.\\
\newpage 
\noindent \textit{Proof:}\\
\noindent\textit{Base case:} Let $n=1$, then it is obvious that $\sum_{k=1}^1 k = 1 = \frac{1\cdot(2)}{2}=1$\\
\textit{Induction step:} Suppose for $n\geq 1$ it is true that 
\begin{align}
    \sum_{k=1}^n k&=\frac{n(n+1)}{2}\nonumber\\ 
    \implies \sum_{k=1}^{n} k + (n+1)&=\frac{n(n+1)}{2} +(n+1)\nonumber\\ 
    &=\frac{n(n+1)}{2} +\frac{2}{2}(n+1)\nonumber\\
    &=\frac{n(n+1)+2(n+1)}{2} \nonumber \\
    &=\frac{(n+1)(n+2)}{2} \nonumber \\
    &=\frac{(n+1)((n+1)+1)}{2} \nonumber
\end{align}
Since $\sum_{k=1}^{n} k + (n+1)=\sum_{k=1}^{n+1} k$ we have shown that arbitrary $n\geq 1$ obeying the rule \textit{implies} that $n+1$ also obeys the rule. This induction step, along with the base case for $n=1$ completes the proof. $\blacksquare$
\smallbreak

\section{A common pitfall when first learning Proof by Induction}
%\noindent Having just covered the principle of mathematical induction, the authors want to emphasize that both of the steps used (i.e. the base case, and inductive step) play an \textit{equal role} in proof of the fact. We emphasize the importance of the \textit{base case} first and then will use an example to show how base case oversights can often reveal themselves in the inductive step. In particular, we emphasize that when choosing the base case, the proof-writer should ensure that the base case is satisfied because of a property that follows from the assumptions about the claim, and not from a property of the \textit{specific} base case that was chosen. \\

%\noindent When choosing the base case above you might notice that we chose $n=1$ instead of $n=0$, partly this is because the claim only pertained to $n>0$, but the other reason for this is because while, $n=0$ satisfies the rule, (i.e. $0(0+1)/2=0$, it satisfies the rule in a trivial way, because the $0$ on the left annihilates the $1/2$ on the right and the sum of $0$ numbers $k$ is itself 0. \\ \\
%\noindent To make this more clear, what if you wanted to prove the obviously false statement that for $n>0, n^3\leq n^2$. \\ \\
%\textit{Bad Base case:}\\
%If we chose $n=1$ to be our base case, we can see that we (\textit{trivially}) satisfy the claim since $1^3=1\leq 1^2 = 1$, if we continued in this way and go on attempting to show the induction step, we may even be able to obtain an (erroneous) proof of this (false) statement. This proof has begun erroneously because the base case was chosen without care. It satisfied the statement about natural numbers, not because it was a natural number who is greater than 0, but because it was the \textit{specific} natural number $1$ (who has the property that all of his powers are the same, $1$).\\

\noindent Let's have a look at another induction proof and  see how a sneaky oversight is made in the proof of the inductive step. The reader should note that \textit{up until} this oversight is made, the proof is valid. \\ \\
\noindent \textit{False theorem:} All horses are the same color.\\ \\
\textit{An Erroneous Proof by induction:}\\ \\
$P(n)$ is the statement: In every set of horses of size $n>0$, all $n$ horses are the same color.\\ \\
\textit{Base Case or $P(1)$}: One horse is the same color as itself. This is true by inspection. \\
\textit{Induction Step}: Assume $P(n)$ for some $n\geq 1$. Since $\{H_1,...,H_n\}$ is a set of $n$ horses, the induction hypothesis applies to this set. Thus, all the horses in this set are the same color.
Since $\{ H_2,...,H_{n+1}\}$ is also a set of $n$ horses, the induction step likewise holds for this set. Thus, all the horses in this set are the same color too. Lastly we note that these sets have horse $H_2$ in common, therefore, all $n+1$ horses in $\{H_1,...,H_n,H_{n+1}\}$ are the same color. $\blacksquare$

%This proof can be thought of as being erroneous for 2 reasons and in some ways these reasons are tied to each other as the reader will see. 
%The first reason this proof is erroneous is because the base case was again chosen without care. It satisfied the statement about collections of horses trivially, because there was only one horse, i.e. in the same way that all things are the same color as themselves, not because it was a collection of $n>0$ horses. In fact, because of the specific base case that we chose, we sneakily manage to \textit{avoid considering} the overlap of two distinct horses $H_1$ and $H_2$ because such a case does not exist for a collection containing only one horse, we only need to confirm that our one horse is the same color as the other horses but there are no "other horses". If we had instead chosen a base case of $n=2$ we would have immediately seen that this statement is not true in general since $\{\text{A Black Stallion}, \text{A White Stallion}\}$ is a collection of $2$ horses that does not satisfy the statement. \\ \\
%The explanation in the above paragraph is a very typical explanation for how choosing a poor representative for the base case can lead to a nonsensical proof. 
So it appears all horses are the same color? Well, there is a mistake in this proof. Can you see what assumption was made without justification?\\ \\
In the last sentence of the \textit{induction step}, we said ``lastly, we note these sets have horse $H_2$ in common." This statement is not necessarily true, is it? Because $H_2 \in \{H_1,...,H_n\}$ is true if and only if $n\geq 2$! The only way we could be sure $H_2 \in \{H_1,...,H_n\}$ is if $n\geq 2$ but we only assumed our $n\geq 1$, meaning $n$ could equal $1$ and then all bets are off about $H_2$ belonging to set $\{H_1,...,H_n\}$. If we take another tack, by instead trying to let our base case be $n=2$ and then make the induction for $n\geq 2$, we would immediately run into a problem. The problem we would encounter is that there are plenty of counter-examples which show the property does not hold for $n=2$ (e.g. $\{\text{A White Stallion},\text{A Black Stallion}\}$ is a collection of $2$ horses that does not satisfy the statement.) so we would be unable to prove our base case. Since we can find even \textit{one} counter-example, it must not be true for \textit{all} sets of horses of size $n$. \\

\noindent Whenever you are asked to prove or disprove a conjecture or claim, if you don't have a feeling about whether the conjecture is right or wrong (true or false) then you should start with some examples.  Either you will find a counter-example (disproving the claim) or you'll start to convince yourself that it's true.  Once you start to believe a conjecture and see a pattern then you can start to try to formulate a proof.  If you find that your proof isn't really coming together, then you can start to ask where it fails which, again, might lead you to a counter-example.
% The heart of our misunderstanding has now been laid bare and we can see that even having somewhat carelessly chosen the base case, it \textit{was} still true that the sets $\{H_1\}$ and $\{H_2\}$ are both collections of $n\geq 1$ horses and in each of those "collection" the horses are all the same color. Our mistake was assuming that the first of these sets should have any more than just $H_1$ which we cannot be sure of (since $n$ could be $1$) since we assumed $n\geq 1$ after making our base step.\\
%The moral of the story is that we must take care when choosing both the base case and making the induction step and we must not assume things before we have shown them!\\ \\

%(If you want even more explaining read my comment in the Ch0\_IntroTopics.tex below this sentence)\\ \\

\section{Why Induction Works: A Rigorous Justification}
\label{sec:WhyInductionWorks}
% Mathematical induction is often introduced through analogy and intuition—such as
% falling dominoes or climbing an infinite ladder. While these images are helpful,
% they hide the logical machinery that makes induction a valid method of proof.
% In this section, we present more formal justification. \\
Upon first reading about induction, many readers may feel that the conclusion doesn't 
follow immediately from the two assumptions (base and inductive steps). Sure the base
case shows $P$ holds for the base $N$, and of course applying modus ponens via the 
induction step to that  base gives $P$ holding for $N+1$ but why should we be allowed to 
continually apply modus ponens without actually doing/saying so?
\begin{align*} 
    P(N) &\text{ and }  P(N)\Rightarrow P(N+1) \ \ \ &\therefore \ P(N+1). \\
    P(N+1) &\text{ and }  P(N+1)\Rightarrow P(N+2) \ \ \ &\therefore \ P(N+2). \\
    &\vdots
\end{align*}
The problem is with what `'..." means. What it really means is an argument by induction. 
But it still remains to show that the argument by induction works. Intuitively, 
we all can see why induction works. But that way of "seeing" might be circular. 
Basically, we "see" that induction holds because of the "..." heuristic above. 
But this assumes that induction works!

Here we will briefly show that once the base case
$P(N)$ has been established, and the induction step 
$\forall \ n \ge N, \  (P(n) \Rightarrow P(n+1))$ has been proven, 
there cannot exist a $k \ge N$ such that $\lnot P(k)$.
We show that repeated application of \emph{modus ponens} is not merely an
informal convenience, but a logically sound consequence of the axioms
underpinning the natural numbers.

\noindent \textit{Proof:} We will prove this via a contraposition argument 
\begin{align*}
    A\Rightarrow B \iff \lnot B \Rightarrow \lnot A
\end{align*}
First, recall that statements of the form $P\Rightarrow Q$ can equivalently be written $Q\lor \lnot P$

\noindent Now, suppose $\exists \ k\geq N \text{ that is the first number for which }\lnot P(k)$ (this is our statement $A$) well then either
\begin{align*}
    k=N \implies \lnot P(N)
\end{align*}
or 
\begin{align*}
    k>N &\implies P(k-1) \ \ \ \ \ \ \ \text{(remember }k \text{ is assumed to be the *first* number that } P \text{ doesn't hold for.)}\\
    &\implies P(k-1)\land \lnot P(k) \\
    &\implies \exists \ n\geq N, \  P(n)\land \lnot P(n+1)
\end{align*}
So from statement $A$ we can infer $\lnot P(N)\lor (\exists \ n\geq N, \  P(n)\land \lnot P(n+1))$ (this is our statement $B$)
So we have established that 
\begin{align*}
    ``\exists \ k\geq N \text{ that is the first number for which }\lnot P(k)" \Longrightarrow ``\lnot P(N)\lor (\exists \ n\geq N, \  P(n)\land \lnot P(n+1))".
\end{align*}
Now, applying contraposition we have 
\begin{align*}
    \lnot(\lnot P(N)\lor (\exists \ n\geq N, \  P(n)\land \lnot P(n+1))) \Longrightarrow \lnot (\exists \ k\geq N \text{ that is the first number for which }\lnot P(k))
\end{align*}
And simplifying the left hand side of this implication with DeMorgan we have
\begin{align*}
    P(N)\land \lnot (\exists \ n\geq N, \  P(n)\land \lnot P(n+1)) \Longrightarrow \lnot (\exists \ k\geq N \text{ that is the first number for which }\lnot P(k))
\end{align*}
Negating the existential quantifier and applying DeMorgan once more we have
\begin{align*}
    P(N)\land (\forall \ n\geq N, \  P(n+1)\lor \lnot P(n)) \Longrightarrow \lnot (\exists \ k\geq N \text{ that is the first number for which }\lnot P(k))
\end{align*}
Finally, rewriting the right side of the conjunction as an implication
\begin{align*}
    P(N)\land (\forall \ n\geq N, \  P(n) \Rightarrow P(n+1)) \Longrightarrow \lnot (\exists \ k\geq N \text{ that is the first number for which }\lnot P(k))
\end{align*}
And there we have it, from the two parts of an induction proof holding (the base and the induction step) we cannot have a $k$ who is the first to break the chain (hence we cannot have any $k$ for which $P$ does not hold).

\newpage
\section{Induction in Fewer Words (from Chong/\.Zak An Intro to Optimization 4 ed.)}
\label{sec:ChongInduction}
The principle of mathematical induction may be stated as follows. \\ \\
Assume that a given property of positive integers, $P(\cdot)$, satisfies the following conditions:
\begin{itemize}
    \item The number 1 possesses this property.
    \item If the number $n$ possesses this property, then the number $n + 1$ possesses it too.
\end{itemize}
The principle of induction states that under these assumptions any positive integer possesses the
property.\\

The principle of induction is easily understood using the following intuitive argument. If the number
$1$ possesses the given property, then the second condition implies that the number $2$ possesses the
property. But, then again, the second condition implies that the number $3$ possesses this property, and so on. The principle of induction is a formal statement of this intuitive reasoning.

%% EVEN MORE EXPLAINING %%
% There are many work-arounds to see that this Theorem is obviously false. Two of these approaches include: 

% 1) redefine a ``set" of ``horses" to always have $n\geq 2$ horses (i.e. a set with a single horse is not a set of horses, it is set containing a single horse) this is sort of a semantically nit-picking approach because it exposes that part of our problem was thinking that a set of "horses" could have only 1 horse to begin with. However this approach doesn't generalize very well because sometimes we \textit{do} want to let a "set" of "horses" include sets with only 1 horse.  This approach is kind of OVERKILL and forces us to rephrase the statement used to show the theorem entirely... (there is nothing wrong with this statement intrinsically... we just need to be careful about what implications we draw from from such a statement when we began by considering a collection with only 1 horse.)

% The other alternative is: 

% 2) \textit{Recognize} that while in the case with one horse, this property is satisfied, for \textit{all} other cases it is not. Then since we want to show for $n>0$ we can show $n=1$ directly as we did but we will NOT treat this as our base case for the induction (We just have to cover $n=1$ case if we are going to make our base case $n=2$ since we wanted to show it holds for $n>0$ ). Having recognized this issue, and having taken care of the earlier numbers below (i.e. $n=1$) the \textit{non-trivial} base case ($n=2$), we will instead set the base case to be $n=2$ and the false-ness of the claimed theorem is immediately apparent as this base has many immediate counter examples to the statement such as the set listed above with different colored stallions. This second approach is almost always the way to go because in induction proofs we are showing an infinite cascade of things are true ($\forall n \in \N, P(n)$), so all you need is one counter example to show the whole thing isn't true ($\exists n\in \N \ni \lnot P(n)$). \\ \\


%\noindent Reading this should be somewhat alarming to the reader and hopefully highlights the importance of choosing a base case that does not \textit{trivially} (i.e. for a reason other than the assumed property, such as the special properties that 1, 0 have in the set of natural numbers, or the special ``pigeonholing" that occurs when we consider 1 horse's similarity to "all other horses" in a set of 1 horses that it belongs to itself!) satisfy the claim we wish to show in induction proofs.


